Date and Time,YYYYMMDD-HHMMSS,20200324-204855,20200325-014254,20200324-220128,20200324-225921,20200325-081236,20200325-091649,20200325-102310,20200325-113640,PLANNED
,Continued Training,,20200324-204855,,,,,,,
Dataset,Sources,"Tweets, Github, FactBase(3/23)","Tweets, Github, FactBase(3/23)",Tweets,Tweets,Tweets,Tweets,Tweets,"Tweets, Github, FactBase(3/24)","Tweets, Github, FactBase(3/24)"
,Unique Chars,67,67,66,66,66,66,66,67,67
,Corpus Length,23321467,23321467,5204704,5204704,5204704,5204704,5204704,23391066,23391066
,Examples,23321447,23321447,5204684,5204684,5204684,5204684,5204684,23391046,23391046
,Validation Split,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05
,Training Examples,22155374,22155374,4944449,4944449,4944449,4944449,4944449,22221493,22221493
,Testing Examples,1166073,1166073,260235,260235,260235,260235,260235,1169553,1169553
Hyperparameters,Learning Rate,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001
,Sequence Length,20,20,20,20,20,20,20,20,20
,Step,1,1,1,1,1,1,1,1,1
,Batch Size,124,124,124,124,124,124,124,124,124
,Shuffle,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE
,Epochs,2,8,6,6,6,6,6,4,4
Train Results,Best Epoch,2,2,4,5,5,4,5,,
,Accuracy,0.65,0.6517,0.6291,0.6289,0.6285,0.6232,0.6268,,
,Loss,1.1698,1.1628,1.2732,1.2736,1.2756,1.2937,1.2794,,
,Training Time (Seconds),,2700,630,630,630,630,700,3586,
Test Results,Accuracy,0.6619,0.66178,0.59347,0.59217,0.59403,0.59142,0.59794,,
,Loss,1.1111,1.1027,1.4322,1.4197,1.424,1.4398,1.4001,,
Model,Input Layer,"GRU(Sequence Length, Unique Chars)",,,"GRU(Sequence Length, Unique Chars)","GRU(Sequence Length, Unique Chars)","GRU(Sequence Length, Unique Chars)","GRU(Sequence Length, Unique Chars)",,
,Layer 1,Unique Chars * 5,,,Unique Chars * 5,Unique Chars * 5,Unique Chars * 5,Unique Chars * 5,,
,Layer 2,Unique Chars * 2,,,Unique Chars * 3,Unique Chars * 2,SELU,SELU,,
,Layer 3,Unique Chars * 2,,,Unique Chars * 2,Unique Chars * 2,Unique Chars * 2,Batch Normalization,,
,Layer 4,Unique Chars,,,Unique Chars,Unique Chars * 2,SELU,Unique Chars * 2,,
,Layer 5,Softmax,,,Softmax,Unique Chars,Unique Chars * 2,SELU,,
,Layer 6,,,,,Softmax,SELU,Batch Normalization,,
,Layer 7,,,,,,Unique Chars,Unique Chars * 2,,
,Layer 8,,,,,,Softmax,SELU,,
,Layer 9,,,,,,,Batch Normalization,,
,Layer 10,,,,,,,Unique Chars,,
,Layer 11,,,,,,,Softmax,,
,Layer 12,,,,,,,,,
Notes,,One of the best models so far. Widening of the network incressed accuracy significantly. Also only using signle step. I think that this is fine and provides more training examples. ,Continued training from the 20200324-204855 model. Training diverged. Accuracy fell significantly. Total epochs trained: 4 ,Same model as 20200324-204855 but only trained on tweets. Using smaller dataset to try and get a model that can do better on training. ,Slightly wider model than 20200324-220128.,Slightly deeper model than 20200324-220128.,Same model as 20200324-220128 Simple mistakes: The dense layers did not have activation functions. Activation functions don't seem to help or have any effect… Very confused.,Adding batch normalization to 20200325-091649. Saw improvements over base model,"Adding more data, activation functions, and batch normalization to 20200324-204855",Exact same model as 20200325-102310 but shuffeled is FALSE
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
PARENT,,,20200324-204855,20200324-204855,20200324-220128,20200324-220128,20200324-220128,20200325-091649,20200324-204855,20200325-102310
,,,,,,,,,,
,,,,,,,,,,
